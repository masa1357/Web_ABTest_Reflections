{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9b501d",
   "metadata": {},
   "source": [
    "# A/B テスト結果の分析\n",
    "\n",
    "baseline（フィードバックA）と ours（フィードバックB）の比較結果を可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378cecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'Python 3.10.12' でセルを実行するには、 ipykernel パッケージが必要です。\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Python 環境の作成</a> および必要なパッケージ。\n",
      "\u001b[1;31mまたは、次のコマンドを使用して 'ipykernel' をインストールします: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# 日本語フォント設定\n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b45384",
   "metadata": {},
   "source": [
    "## 1. データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果ファイルを読み込み\n",
    "results_path = Path(\"results/abtest_results.jsonl\")\n",
    "\n",
    "if not results_path.exists():\n",
    "    print(f\"ファイルが見つかりません: {results_path}\")\n",
    "else:\n",
    "    # JSON Lines を DataFrame に変換\n",
    "    df = pd.read_json(results_path, lines=True)\n",
    "    print(f\"読み込みデータ: {len(df)} 行\")\n",
    "    print(f\"\\n列一覧: {list(df.columns)}\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16218c",
   "metadata": {},
   "source": [
    "## 2. 評価値を標準化（A有利→正、B有利→負）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RATING_SCALE のマッピング\n",
    "# \"A が強く良い\", \"A がやや良い\", \"どちらとも言えない\", \"B がやや良い\", \"B が強く良い\"\n",
    "rating_to_score = {\n",
    "    \"A が強く良い\": 2,\n",
    "    \"A がやや良い\": 1,\n",
    "    \"どちらとも言えない\": 0,\n",
    "    \"B がやや良い\": -1,\n",
    "    \"B が強く良い\": -2,\n",
    "}\n",
    "\n",
    "# 評価項目\n",
    "rating_fields = [\n",
    "    \"usefulness\",\n",
    "    \"readability\",\n",
    "    \"persuasiveness\",\n",
    "    \"actionability\",\n",
    "    \"hallucination\",\n",
    "    \"overall\",\n",
    "]\n",
    "\n",
    "# 各フィールドを数値スコアに変換\n",
    "for field in rating_fields:\n",
    "    df[f\"{field}_score\"] = df[field].map(rating_to_score)\n",
    "\n",
    "print(\"変換完了\")\n",
    "df[[\"usefulness\", \"usefulness_score\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39aad54",
   "metadata": {},
   "source": [
    "## 3. 全体の勝敗集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32915bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各フィールドごとの勝敗を集計\n",
    "results_summary = {}\n",
    "\n",
    "for field in rating_fields:\n",
    "    score_col = f\"{field}_score\"\n",
    "\n",
    "    a_wins = (df[score_col] > 0).sum()\n",
    "    ties = (df[score_col] == 0).sum()\n",
    "    b_wins = (df[score_col] < 0).sum()\n",
    "\n",
    "    results_summary[field] = {\n",
    "        \"Baseline (A) 勝利\": a_wins,\n",
    "        \"同程度\": ties,\n",
    "        \"Ours (B) 勝利\": b_wins,\n",
    "    }\n",
    "\n",
    "summary_df = pd.DataFrame(results_summary).T\n",
    "print(\"\\n=== 評価項目ごとの勝敗集計 ===\")\n",
    "print(summary_df)\n",
    "print(f\"\\n総回答数: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c4c3a",
   "metadata": {},
   "source": [
    "## 4. 平均スコアの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da317d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各フィールドの平均スコアを計算（正=A有利、負=B有利）\n",
    "avg_scores = {}\n",
    "for field in rating_fields:\n",
    "    score_col = f\"{field}_score\"\n",
    "    avg_scores[field] = df[score_col].mean()\n",
    "\n",
    "print(\"\\n=== 平均スコア（正=A有利、負=B有利） ===\")\n",
    "for field, score in avg_scores.items():\n",
    "    direction = \"A有利\" if score > 0 else \"B有利\" if score < 0 else \"同程度\"\n",
    "    print(f\"{field:20s}: {score:+.3f}  ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23f0b7",
   "metadata": {},
   "source": [
    "## 5. 棒グラフ：項目ごとの勝敗（積み上げ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語ラベル\n",
    "field_labels = {\n",
    "    \"usefulness\": \"有用性\",\n",
    "    \"readability\": \"可読性\",\n",
    "    \"persuasiveness\": \"説得力\",\n",
    "    \"actionability\": \"行動可能性\",\n",
    "    \"hallucination\": \"信頼性\",\n",
    "    \"overall\": \"総合評価\",\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(rating_fields))\n",
    "width = 0.6\n",
    "\n",
    "a_wins_list = [summary_df.loc[f, \"Baseline (A) 勝利\"] for f in rating_fields]\n",
    "ties_list = [summary_df.loc[f, \"同程度\"] for f in rating_fields]\n",
    "b_wins_list = [summary_df.loc[f, \"Ours (B) 勝利\"] for f in rating_fields]\n",
    "\n",
    "ax.bar(x, a_wins_list, width, label=\"Baseline (A) 勝利\", color=\"#FF6B6B\", alpha=0.8)\n",
    "ax.bar(x, ties_list, width, bottom=a_wins_list, label=\"同程度\", color=\"#FFD93D\", alpha=0.8)\n",
    "ax.bar(x, b_wins_list, width, bottom=np.array(a_wins_list) + np.array(ties_list),\n",
    "       label=\"Ours (B) 勝利\", color=\"#6BCB77\", alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"回答数\", fontsize=12)\n",
    "ax.set_title(\"A/B テスト結果：項目ごとの勝敗\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([field_labels[f] for f in rating_fields], fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/01_wins_by_field.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3716c4",
   "metadata": {},
   "source": [
    "## 6. 折れ線グラフ：平均スコアの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "scores = [avg_scores[f] for f in rating_fields]\n",
    "labels = [field_labels[f] for f in rating_fields]\n",
    "\n",
    "colors = [\"#FF6B6B\" if s > 0 else \"#6BCB77\" if s < 0 else \"#FFD93D\" for s in scores]\n",
    "\n",
    "ax.plot(labels, scores, marker=\"o\", linewidth=2.5, markersize=10, color=\"#4A90E2\")\n",
    "ax.bar(labels, scores, alpha=0.5, color=colors)\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_ylabel(\"平均スコア（正=A有利、負=B有利）\", fontsize=12)\n",
    "ax.set_title(\"A/B テスト結果：平均スコア\", fontsize=14, fontweight=\"bold\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# y軸範囲を調整\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# スコア値を各点に表示\n",
    "for i, (label, score) in enumerate(zip(labels, scores)):\n",
    "    ax.text(i, score + 0.1, f\"{score:+.2f}\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.xticks(rotation=15, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/02_average_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97649be1",
   "metadata": {},
   "source": [
    "## 7. 全体の勝敗（総合評価のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a140fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "overall_data = summary_df.loc[\"overall\"].values\n",
    "labels = summary_df.loc[\"overall\"].index\n",
    "colors_pie = [\"#FF6B6B\", \"#FFD93D\", \"#6BCB77\"]\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    overall_data,\n",
    "    labels=labels,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors_pie,\n",
    "    startangle=90,\n",
    "    textprops={\"fontsize\": 11}\n",
    ")\n",
    "\n",
    "# パーセンテージのテキストを太字に\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color(\"white\")\n",
    "    autotext.set_fontweight(\"bold\")\n",
    "    autotext.set_fontsize(12)\n",
    "\n",
    "ax.set_title(\"総合評価の勝敗（全体）\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/03_overall_pie.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b086a",
   "metadata": {},
   "source": [
    "## 8. 統計的有意性の簡易検定（符号検定）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"\\n=== 符号検定（統計的有意性） ===\")\n",
    "print(\"帰無仮説: A と B に差がない\\n\")\n",
    "\n",
    "for field in rating_fields:\n",
    "    score_col = f\"{field}_score\"\n",
    "    # 同程度を除いて検定\n",
    "    scores_nonzero = df[df[score_col] != 0][score_col].values\n",
    "\n",
    "    if len(scores_nonzero) > 0:\n",
    "        # 符号検定（2項検定）\n",
    "        a_count = (scores_nonzero > 0).sum()\n",
    "        n = len(scores_nonzero)\n",
    "        p_value = stats.binomtest(a_count, n, 0.5, alternative=\"two-sided\").pvalue\n",
    "\n",
    "        significance = \"*\" if p_value < 0.05 else \"\"\n",
    "        print(f\"{field_labels[field]:10s}: p={p_value:.4f}  {significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba05d31",
   "metadata": {},
   "source": [
    "## 9. 参加者属性別の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 九州大学学生の有無別\n",
    "print(\"\\n=== 九州大学学生の有無別 ===\")\n",
    "if \"kyushu_student\" in df.columns:\n",
    "    for group in df[\"kyushu_student\"].unique():\n",
    "        subset = df[df[\"kyushu_student\"] == group]\n",
    "        avg_score = subset[\"overall_score\"].mean()\n",
    "        print(f\"{group}: n={len(subset)}, 平均スコア={avg_score:+.3f}\")\n",
    "\n",
    "# 情報科学受講経験別\n",
    "print(\"\\n=== 情報科学受講経験別 ===\")\n",
    "if \"info_course_taken\" in df.columns:\n",
    "    for group in df[\"info_course_taken\"].unique():\n",
    "        subset = df[df[\"info_course_taken\"] == group]\n",
    "        avg_score = subset[\"overall_score\"].mean()\n",
    "        print(f\"{group}: n={len(subset)}, 平均スコア={avg_score:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69d174",
   "metadata": {},
   "source": [
    "## 10. サマリーテーブルをCSVに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7baa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果サマリーをCSVに保存\n",
    "summary_df.to_csv(\"results/summary_wins.csv\")\n",
    "\n",
    "# 平均スコアをCSVに保存\n",
    "avg_df = pd.DataFrame({\n",
    "    \"評価項目\": [field_labels[f] for f in rating_fields],\n",
    "    \"平均スコア\": [avg_scores[f] for f in rating_fields],\n",
    "})\n",
    "avg_df.to_csv(\"results/average_scores.csv\", index=False)\n",
    "\n",
    "print(\"✓ results/summary_wins.csv\")\n",
    "print(\"✓ results/average_scores.csv\")\n",
    "print(\"\\n分析完了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553d17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
